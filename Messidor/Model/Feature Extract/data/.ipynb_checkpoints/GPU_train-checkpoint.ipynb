{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cd7ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14472ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch import nn,optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms \n",
    "from torchvision import models as m\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import gc  #garbage collector\n",
    "import json\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "from torchinfo import summary\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5f22321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa6f0140a50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.manual_seed(1412)\n",
    "torch.cuda.manual_seed_all(1412)\n",
    "torch.manual_seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c1219c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a2ac019",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_bn_ =m.vgg16_bn() \n",
    "resnet18_ = m.resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0785ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet3(nn.Module):\n",
    "    def __init__(self,in_features=3):\n",
    "        super().__init__()\n",
    "#         self.conv1=nn.Sequential( BasicConv2d(in_ = in_features,out_=32,kernel_size=3,stride=1,padding=1)\n",
    "#                                  ,BasicConv2d(in_ = 32,out_=64,kernel_size=3,stride=1,padding=1)\n",
    "#                                 ,nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False))\n",
    "#         self.conv1=nn.Sequential( BasicConv2d(in_ = in_features,out_=32,kernel_size=3,stride=1,padding=1)\n",
    "#                                  ,BasicConv2d(in_ = 32,out_=64,kernel_size=3,stride=1,padding=1)\n",
    "#                                 ,nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False))\n",
    "    \n",
    "#         self.conv1 = nn.Sequential(BasicConv2d_two(in_features,32,64,kernel_size=3,stride=1,padding=1))\n",
    "        self.conv1 = nn.Sequential(BasicConv2d_two( in_features, 32, 32,kernel_size=3,stride=1,padding=1))\n",
    "        self.conv2 = nn.Sequential(BasicConv2d_two( 32, 64, 64,kernel_size=3,stride=1,padding=1))\n",
    "#         self.block1=vgg16_bn_.features[7:14]\n",
    "        self.block1=resnet18_.layer2\n",
    "        self.block2=resnet18_.layer3\n",
    "        self.avgpool=vgg16_bn_.avgpool\n",
    "        self.classifer=nn.Sequential( nn.Linear(12544, 256, bias=True)\n",
    "                                     ,nn.BatchNorm1d(256)\n",
    "                                     ,nn.Dropout(p=0.5)\n",
    "                                     ,nn.Linear(256,128, bias=True)\n",
    "                                     ,nn.BatchNorm1d(128)\n",
    "                                     ,nn.Dropout(p=0.5)\n",
    "                                    )\n",
    "        self.fc = nn.Linear(128,4)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x= self.conv2(x)\n",
    "        x= self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.avgpool(x)\n",
    "        x=x.view(-1,12544)\n",
    "        x = self.classifer(x)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0c95864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv2d_two(nn.Module):\n",
    "    def __init__(self,in_,out_,f1,**kwargs):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_,out_, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_, f1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "          )\n",
    "            \n",
    "            \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "473d6c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net3 = MyNet3()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b01ca28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyNet3                                   [120, 4]                  --\n",
       "├─Sequential: 1-1                        [120, 32, 112, 112]       --\n",
       "│    └─BasicConv2d_two: 2-1              [120, 32, 112, 112]       --\n",
       "│    │    └─Sequential: 3-1              [120, 32, 112, 112]       10,272\n",
       "├─Sequential: 1-2                        [120, 64, 56, 56]         --\n",
       "│    └─BasicConv2d_two: 2-2              [120, 64, 56, 56]         --\n",
       "│    │    └─Sequential: 3-2              [120, 64, 56, 56]         55,680\n",
       "├─Sequential: 1-3                        [120, 128, 28, 28]        --\n",
       "│    └─BasicBlock: 2-3                   [120, 128, 28, 28]        --\n",
       "│    │    └─Conv2d: 3-3                  [120, 128, 28, 28]        73,728\n",
       "│    │    └─BatchNorm2d: 3-4             [120, 128, 28, 28]        256\n",
       "│    │    └─ReLU: 3-5                    [120, 128, 28, 28]        --\n",
       "│    │    └─Conv2d: 3-6                  [120, 128, 28, 28]        147,456\n",
       "│    │    └─BatchNorm2d: 3-7             [120, 128, 28, 28]        256\n",
       "│    │    └─Sequential: 3-8              [120, 128, 28, 28]        8,448\n",
       "│    │    └─ReLU: 3-9                    [120, 128, 28, 28]        --\n",
       "│    └─BasicBlock: 2-4                   [120, 128, 28, 28]        --\n",
       "│    │    └─Conv2d: 3-10                 [120, 128, 28, 28]        147,456\n",
       "│    │    └─BatchNorm2d: 3-11            [120, 128, 28, 28]        256\n",
       "│    │    └─ReLU: 3-12                   [120, 128, 28, 28]        --\n",
       "│    │    └─Conv2d: 3-13                 [120, 128, 28, 28]        147,456\n",
       "│    │    └─BatchNorm2d: 3-14            [120, 128, 28, 28]        256\n",
       "│    │    └─ReLU: 3-15                   [120, 128, 28, 28]        --\n",
       "├─Sequential: 1-4                        [120, 256, 14, 14]        --\n",
       "│    └─BasicBlock: 2-5                   [120, 256, 14, 14]        --\n",
       "│    │    └─Conv2d: 3-16                 [120, 256, 14, 14]        294,912\n",
       "│    │    └─BatchNorm2d: 3-17            [120, 256, 14, 14]        512\n",
       "│    │    └─ReLU: 3-18                   [120, 256, 14, 14]        --\n",
       "│    │    └─Conv2d: 3-19                 [120, 256, 14, 14]        589,824\n",
       "│    │    └─BatchNorm2d: 3-20            [120, 256, 14, 14]        512\n",
       "│    │    └─Sequential: 3-21             [120, 256, 14, 14]        33,280\n",
       "│    │    └─ReLU: 3-22                   [120, 256, 14, 14]        --\n",
       "│    └─BasicBlock: 2-6                   [120, 256, 14, 14]        --\n",
       "│    │    └─Conv2d: 3-23                 [120, 256, 14, 14]        589,824\n",
       "│    │    └─BatchNorm2d: 3-24            [120, 256, 14, 14]        512\n",
       "│    │    └─ReLU: 3-25                   [120, 256, 14, 14]        --\n",
       "│    │    └─Conv2d: 3-26                 [120, 256, 14, 14]        589,824\n",
       "│    │    └─BatchNorm2d: 3-27            [120, 256, 14, 14]        512\n",
       "│    │    └─ReLU: 3-28                   [120, 256, 14, 14]        --\n",
       "├─AdaptiveAvgPool2d: 1-5                 [120, 256, 7, 7]          --\n",
       "├─Sequential: 1-6                        [120, 128]                --\n",
       "│    └─Linear: 2-7                       [120, 256]                3,211,520\n",
       "│    └─BatchNorm1d: 2-8                  [120, 256]                512\n",
       "│    └─Dropout: 2-9                      [120, 256]                --\n",
       "│    └─Linear: 2-10                      [120, 128]                32,896\n",
       "│    └─BatchNorm1d: 2-11                 [120, 128]                256\n",
       "│    └─Dropout: 2-12                     [120, 128]                --\n",
       "├─Linear: 1-7                            [120, 4]                  516\n",
       "==========================================================================================\n",
       "Total params: 5,936,932\n",
       "Trainable params: 5,936,932\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 243.55\n",
       "==========================================================================================\n",
       "Input size (MB): 72.25\n",
       "Forward/backward pass size (MB): 10694.25\n",
       "Params size (MB): 23.75\n",
       "Estimated Total Size (MB): 10790.25\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(net3,input_size=(120,3,224,224),depth=3,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f2c85dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience = 5, tol = 0.0005): #惯例地定义我们所需要的一切变量/属性\\\n",
    "        #当连续patience次迭代时，这一轮迭代的损失与历史最低损失之间的差值小于阈值时\n",
    "        #就触发提前停止\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.tol = tol #tolerance，累积5次都低于tol才会触发停止\n",
    "        self.counter = 0 #计数，计算现在已经累积了counter次\n",
    "        self.lowest_loss = None\n",
    "        self.early_stop = True #True - 提前停止，False - 不要提前停止\n",
    "    \n",
    "    def __call__(self,val_loss):\n",
    "        if self.lowest_loss == None: #这是第一轮迭代\n",
    "            self.lowest_loss = val_loss\n",
    "        elif self.lowest_loss - val_loss > self.tol:\n",
    "            self.lowest_loss = val_loss\n",
    "            self.counter = 0\n",
    "        elif self.lowest_loss - val_loss < self.tol:\n",
    "            self.counter += 1\n",
    "            print(\"\\t NOTICE: Early stopping counter {} of {}\".format(self.counter,self.patience))\n",
    "            if self.counter >= self.patience:\n",
    "                print('\\t NOTICE: Early Stopping Actived')\n",
    "                self.early_stop = True\n",
    "        return self.early_stop\n",
    "        #这一轮迭代的损失与历史最低损失之间的差 - 阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02d22e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IterOnce(net,criterion,opt,x,y):\n",
    "    \"\"\"\n",
    "    对模型进行一次迭代的函数\n",
    "    \n",
    "    net: 实例化后的架构\n",
    "    criterion: 损失函数\n",
    "    opt: 优化算法\n",
    "    x: 这一个batch中所有的样本\n",
    "    y: 这一个batch中所有样本的真实标签\n",
    "    \"\"\"\n",
    "    sigma = net.forward(x)\n",
    "    loss = criterion(sigma,y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad(set_to_none=True) #比起设置梯度为0，让梯度为None会更节约内存\n",
    "    yhat = torch.max(sigma,1)[1]\n",
    "    correct = torch.sum(yhat == y)\n",
    "    return correct,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4da33bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestOnce(net,criterion,x,y):\n",
    "    \"\"\"\n",
    "    对一组数据进行测试并输出测试结果的函数\n",
    "    \n",
    "    net: 经过训练后的架构\n",
    "    criterion：损失函数\n",
    "    x：要测试的数据的所有样本\n",
    "    y：要测试的数据的真实标签\n",
    "    \"\"\"\n",
    "    #对测试，一定要阻止计算图追踪\n",
    "    #这样可以节省很多内存，加速运算\n",
    "    with torch.no_grad(): \n",
    "        sigma = net.forward(x)\n",
    "        loss = criterion(sigma,y)\n",
    "        yhat = torch.max(sigma,1)[1]\n",
    "        correct = torch.sum(yhat == y)\n",
    "    return correct,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bc75f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_test(net,batchdata,criterion,opt,epochs,tol,modelname,PATH):\n",
    "    \"\"\"\n",
    "    对模型进行训练，并在每个epoch后输出训练集和测试集上的准确率/损失\n",
    "    以实现对模型的监控\n",
    "    实现模型的保存\n",
    "    \n",
    "    参数说明：\n",
    "    net: 实例化后的网络\n",
    "    batchdata：使用Dataloader分割后的训练数据\n",
    "    testdata：使用Dataloader分割后的测试数据\n",
    "    criterion：所使用的损失函数\n",
    "    opt：所使用的优化算法\n",
    "    epochs：一共要使用完整数据集epochs次\n",
    "    tol：提前停止时测试集上loss下降的阈值，连续5次loss下降不超过tol就会触发提前停止\n",
    "    modelname：现在正在运行的模型名称，用于保存权重时作为文件名\n",
    "    PATH：将权重文件保存在path目录下\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    SamplePerEpoch = batchdata.dataset.__len__() #整个epoch里有多少个样本\n",
    "    allsamples = SamplePerEpoch*epochs\n",
    "    trainedsamples = 0\n",
    "    trainlosslist = []\n",
    "    \n",
    "#     early_stopping = EarlyStopping(tol=tol)\n",
    "    highestacc = None\n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        net.train()\n",
    "        correct_train = 0\n",
    "        loss_train = 0\n",
    "        for batch_idx, (x, y) in enumerate(batchdata):\n",
    "             #non_blocking 非阻塞=True，表示允许多个线程同时占用一个资源\n",
    "            #一般来说，一段代码/数据占用一部分计算资源时，该资源是不对其他代码/数据开放的\n",
    "            #此时被占用的资源叫做临界资源，正在运行的代码/数据被叫做临界区\n",
    "            #设置non_blocking=True，相当于允许多个代码在临界资源上运行\n",
    "            #可以加速运算\n",
    "            x = x.to(device,non_blocking=True)\n",
    "            y = y.to(device,non_blocking=True).view(x.shape[0])\n",
    "           \n",
    "            correct, loss = IterOnce(net,criterion,opt,x,y)\n",
    "            trainedsamples += x.shape[0]\n",
    "            loss_train += loss\n",
    "            correct_train += correct\n",
    "            \n",
    "            if (batch_idx+1) % 125 == 0:\n",
    "                #现在进行到了哪个epoch\n",
    "                #现在训练到了多少个样本\n",
    "                #总共要训练多少个样本\n",
    "                #现在的训练的样本占总共需要训练的样本的百分比\n",
    "                print('Epoch{}:[{}/{}({:.0f}%)]'.format(epoch\n",
    "                                                       ,trainedsamples\n",
    "                                                       ,allsamples\n",
    "                                                       ,100*trainedsamples/allsamples))\n",
    "            \n",
    "        TrainAccThisEpoch = float(correct_train*100)/SamplePerEpoch\n",
    "        TrainLossThisEpoch = float(loss_train)/SamplePerEpoch #平均每个样本上的损失\n",
    "        trainlosslist.append(TrainLossThisEpoch)\n",
    "    \n",
    "       #清理GPU内存\n",
    "        del x,y,correct,loss,correct_train,loss_train\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "      \n",
    "    \n",
    "#         #每次训练完一个epoch，就在测试集上验证一下模型现在的效果\n",
    "#         net.eval()\n",
    "#         loss_test = 0\n",
    "#         correct_test = 0\n",
    "#         loss_test = 0\n",
    "#         TestSample = testdata.dataset.__len__()\n",
    "\n",
    "#         for x,y in testdata:\n",
    "#             x = x.to(device,non_blocking=True)\n",
    "#             y = y.to(device,non_blocking=True).view(x.shape[0])\n",
    "           \n",
    "#             correct, loss = TestOnce(net,criterion,x,y)\n",
    "#             loss_test += loss\n",
    "#             correct_test += correct\n",
    "\n",
    "#         TestAccThisEpoch = float(correct_test * 100)/TestSample\n",
    "#         TestLossThisEpoch = float(loss_test )/TestSample\n",
    "#         testlosslist.append(TestLossThisEpoch)\n",
    "        \n",
    "#         del x,y,correct,loss,correct_test, loss_test\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "        \n",
    "        #对每一个epoch，打印训练和测试的结果\n",
    "        #训练集上的损失，测试集上的损失，训练集上的准确率，测试集上的准确率\n",
    "        print(\"\\t Train Loss:{:.6f}, Train Acc:{:.3f}%\".format(TrainLossThisEpoch\n",
    "                                                            ,TrainAccThisEpoch\n",
    "                                                                                 ))\n",
    "        \n",
    "        #如果测试集准确率出现新高/测试集loss出现新低，那我会保存现在的这一组权重\n",
    "        if highestacc == None: #首次进行测试\n",
    "            highestacc = TrainAccThisEpoch\n",
    "        if highestacc < TrainAccThisEpoch:\n",
    "            highestacc = TrainAccThisEpoch\n",
    "            torch.save(net.state_dict(),os.path.join(PATH,modelname+\".pt\"))\n",
    "#             torch.save(net.state_dict(), os.path.join(PATH, str(modelname) + \".pt\"))\n",
    "            print(\"\\t Weight Saved\")\n",
    "        \n",
    "#         #提前停止\n",
    "#         early_stop = early_stopping(TestLossThisEpoch)\n",
    "#         if early_stop == \"True\":\n",
    "#             break\n",
    "            \n",
    "    print(\"Complete\")\n",
    "    return trainlosslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "692ba414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_split_data(root: str, val_rate: float = 0.3):\n",
    "    random.seed(420)  # 保证随机结果可复现\n",
    "    assert os.path.exists(root), \"dataset root: {} does not exist.\".format(root)\n",
    "\n",
    "    # 遍历文件夹，一个文件夹对应一个类别\n",
    "    flower_class = [cla for cla in os.listdir(root) if os.path.isdir(os.path.join(root, cla))]\n",
    "    # 排序，保证各平台顺序一致\n",
    "    flower_class.sort()\n",
    "    # 生成类别名称以及对应的数字索引\n",
    "    class_indices = dict((k, v) for v, k in enumerate(flower_class))\n",
    "    json_str = json.dumps(dict((val, key) for key, val in class_indices.items()), indent=4)\n",
    "    with open('class_indices.json', 'w') as json_file:\n",
    "        json_file.write(json_str)\n",
    "\n",
    "    train_images_path = []  # 存储训练集的所有图片路径\n",
    "    train_images_label = []  # 存储训练集图片对应索引信息\n",
    "    val_images_path = []  # 存储验证集的所有图片路径\n",
    "    val_images_label = []  # 存储验证集图片对应索引信息\n",
    "    every_class_num = []  # 存储每个类别的样本总数\n",
    "    supported = [\".jpg\", \".JPG\", \".png\", \".PNG\"]  # 支持的文件后缀类型\n",
    "    # 遍历每个文件夹下的文件\n",
    "    for cla in flower_class:\n",
    "        cla_path = os.path.join(root, cla)\n",
    "        # 遍历获取supported支持的所有文件路径\n",
    "        images = [os.path.join(root, cla, i) for i in os.listdir(cla_path)\n",
    "                  if os.path.splitext(i)[-1] in supported]\n",
    "        # 排序，保证各平台顺序一致\n",
    "        images.sort()\n",
    "        # 获取该类别对应的索引\n",
    "        image_class = class_indices[cla]\n",
    "        # 记录该类别的样本数量\n",
    "        every_class_num.append(len(images))\n",
    "        # 按比例随机采样验证样本\n",
    "#         val_path = random.sample(images, k=int(len(images) * val_rate))\n",
    "\n",
    "        for img_path in images:\n",
    "#             if img_path in val_path:  # 如果该路径在采样的验证集样本中则存入验证集\n",
    "#                 val_images_path.append(img_path)\n",
    "#                 val_images_label.append(image_class)\n",
    "#             else:  # 否则存入训练集\n",
    "            train_images_path.append(img_path)\n",
    "            train_images_label.append(image_class)\n",
    "\n",
    "    print(\"{} images were found in the dataset.\".format(sum(every_class_num)))\n",
    "    print(\"{} images for training.\".format(len(train_images_path)))\n",
    "   \n",
    "    assert len(train_images_path) > 0, \"number of training images must greater than 0.\"\n",
    "  \n",
    "    \n",
    "\n",
    "    return train_images_path, train_images_label, val_images_path, val_images_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1fcedbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet():\n",
    "    \"\"\"自定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, images_path: list, images_class: list, transform=None):\n",
    "        self.images_path = images_path\n",
    "        self.images_class = images_class\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_path)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img = Image.open(self.images_path[item])\n",
    "        # RGB为彩色图片，L为灰度图片\n",
    "        if img.mode != 'RGB':\n",
    "            raise ValueError(\"image: {} isn't RGB mode.\".format(self.images_path[item]))\n",
    "        label = self.images_class[item]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def __getitem__(self, item):\n",
    "#         img = cv2.imread(self.images_path[item], cv2.IMREAD_GRAYSCALE)\n",
    "#         if img is None:\n",
    "#             raise ValueError(\"Failed to read image: {}\".format(self.images_path[item]))\n",
    "#         label = self.images_class[item]\n",
    "    \n",
    "#         if self.transform is not None:\n",
    "#             img = self.transform(img)\n",
    "    \n",
    "#         return img, label\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        # 官方实现的default_collate可以参考\n",
    "        # https://github.com/pytorch/pytorch/blob/67b7e751e6b5931a9f45274653f4f653a4e6cdf6/torch/utils/data/_utils/collate.py\n",
    "        images, labels = tuple(zip(*batch))\n",
    "\n",
    "        images = torch.stack(images, dim=0)\n",
    "        labels = torch.as_tensor(labels)\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38ba4aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/ubuntu/fy/diabetic retinopathy/feature selection/data/seg_VB_tofeature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4631be33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200 images were found in the dataset.\n",
      "1200 images for training.\n"
     ]
    }
   ],
   "source": [
    "train_images_path, train_images_label, val_images_path, val_images_label = read_split_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6a8a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "data_transform = {\n",
    "        \"train\": transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        \"val\": transforms.Compose([\n",
    "#                                  transforms.Resize(int(img_size * 1.143)),\n",
    "#                                    transforms.CenterCrop(img_size),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])}\n",
    "\n",
    "# 实例化训练数据集\n",
    "train_dataset = MyDataSet(images_path=train_images_path,\n",
    "                              images_class=train_images_label,\n",
    "                               transform=data_transform[\"train\"]\n",
    "                         )\n",
    "\n",
    "# # 实例化验证数据集\n",
    "# val_dataset = MyDataSet(images_path=val_images_path,\n",
    "#                             images_class=val_images_label,\n",
    "#                            transform=data_transform[\"val\"]\n",
    "#                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ea1e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_procedure(net,epochs,bs,modelname, PATH, lr=0.001,alpha=0.99,gamma=0,wd=0,tol=10**(-5)):\n",
    "    \n",
    "    torch.manual_seed(1412)\n",
    "    \n",
    "    #分割数据\n",
    "    batchdata = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=bs,\n",
    "                                               shuffle=True,\n",
    "                                               pin_memory=True,\n",
    "                                               drop_last=True,\n",
    "                                               collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "#     testdata =torch.utils.data.DataLoader(val_dataset,\n",
    "#                                              batch_size=bs,\n",
    "#                                              drop_last=True,\n",
    "#                                              shuffle=False,\n",
    "#                                              pin_memory=True,\n",
    "                                             \n",
    "#                                              collate_fn=val_dataset.collate_fn)\n",
    "    \n",
    "    #损失函数，优化算法\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\") #进行损失函数计算时，最后输出结果的计算模式\n",
    "    opt = optim.RMSprop(net.parameters(),lr=lr\n",
    "                        ,alpha=alpha,momentum=gamma,weight_decay=wd)\n",
    "    \n",
    "    #训练与测试\n",
    "    trainloss = fit_test(net,batchdata,criterion,opt,epochs,tol,modelname,PATH)\n",
    "    \n",
    "    return trainloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634c00e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d63373c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#绘图函数\n",
    "def plotloss(trainloss, testloss):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(trainloss, color=\"red\", label=\"Trainloss\")\n",
    "    plt.plot(testloss, color=\"orange\", label=\"Testloss\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d141f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aede31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#实例化\n",
    "#trainloss, testloss = full_procedure(XXXX)\n",
    "#plotloss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "725b2eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet3_0.001_train_2\n",
      "\t Train Loss:1.935333, Train Acc:46.250%\n",
      "\t Train Loss:1.330536, Train Acc:40.750%\n",
      "\t Train Loss:1.314603, Train Acc:41.750%\n",
      "\t Train Loss:1.263438, Train Acc:42.917%\n",
      "\t Train Loss:1.288392, Train Acc:43.500%\n",
      "\t Train Loss:1.264763, Train Acc:43.333%\n",
      "\t Train Loss:1.250730, Train Acc:43.250%\n",
      "\t Train Loss:1.212415, Train Acc:43.333%\n",
      "\t Train Loss:1.260637, Train Acc:43.333%\n",
      "\t Train Loss:1.234235, Train Acc:43.333%\n",
      "\t Train Loss:1.207061, Train Acc:41.917%\n",
      "\t Train Loss:1.214319, Train Acc:43.250%\n",
      "\t Train Loss:1.215269, Train Acc:42.083%\n",
      "\t Train Loss:1.188710, Train Acc:44.417%\n",
      "\t Train Loss:1.164582, Train Acc:44.667%\n",
      "\t Train Loss:1.173631, Train Acc:45.417%\n",
      "\t Train Loss:1.178403, Train Acc:45.500%\n",
      "\t Train Loss:1.158176, Train Acc:46.250%\n",
      "\t Train Loss:1.160495, Train Acc:46.083%\n",
      "\t Train Loss:1.180478, Train Acc:43.750%\n",
      "\t Train Loss:1.159051, Train Acc:46.833%\n",
      "\t Weight Saved\n",
      "\t Train Loss:1.152147, Train Acc:45.833%\n",
      "\t Train Loss:1.162567, Train Acc:46.083%\n",
      "\t Train Loss:1.132934, Train Acc:47.583%\n",
      "\t Weight Saved\n",
      "\t Train Loss:1.125926, Train Acc:47.333%\n",
      "\t Train Loss:1.134989, Train Acc:47.333%\n",
      "\t Train Loss:1.119528, Train Acc:49.417%\n",
      "\t Weight Saved\n",
      "\t Train Loss:1.133289, Train Acc:46.500%\n",
      "\t Train Loss:1.097073, Train Acc:48.333%\n",
      "\t Train Loss:1.110350, Train Acc:46.833%\n",
      "\t Train Loss:1.067156, Train Acc:50.417%\n",
      "\t Weight Saved\n",
      "\t Train Loss:1.074649, Train Acc:50.167%\n",
      "\t Train Loss:1.039763, Train Acc:52.417%\n",
      "\t Weight Saved\n",
      "\t Train Loss:1.047670, Train Acc:50.833%\n",
      "\t Train Loss:1.070945, Train Acc:50.167%\n",
      "\t Train Loss:1.010342, Train Acc:54.000%\n",
      "\t Weight Saved\n",
      "\t Train Loss:1.039799, Train Acc:51.500%\n",
      "\t Train Loss:1.015067, Train Acc:51.750%\n",
      "\t Train Loss:0.985071, Train Acc:55.167%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.974215, Train Acc:53.250%\n",
      "\t Train Loss:0.925742, Train Acc:56.167%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.933567, Train Acc:56.250%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.930191, Train Acc:57.417%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.874295, Train Acc:59.333%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.861499, Train Acc:60.750%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.841181, Train Acc:60.167%\n",
      "\t Train Loss:0.800429, Train Acc:62.583%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.807838, Train Acc:63.417%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.790165, Train Acc:63.833%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.721755, Train Acc:68.000%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.673167, Train Acc:69.833%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.614709, Train Acc:70.417%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.568617, Train Acc:75.417%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.582007, Train Acc:72.500%\n",
      "\t Train Loss:0.465751, Train Acc:78.667%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.502953, Train Acc:76.917%\n",
      "\t Train Loss:0.429605, Train Acc:79.667%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.335515, Train Acc:84.667%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.343325, Train Acc:84.500%\n",
      "\t Train Loss:0.299466, Train Acc:85.583%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.385109, Train Acc:82.583%\n",
      "\t Train Loss:0.256222, Train Acc:86.667%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.175132, Train Acc:91.083%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.164588, Train Acc:91.083%\n",
      "\t Train Loss:0.156265, Train Acc:90.250%\n",
      "\t Train Loss:0.126107, Train Acc:91.083%\n",
      "\t Train Loss:0.178703, Train Acc:89.250%\n",
      "\t Train Loss:0.097961, Train Acc:92.667%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.113040, Train Acc:92.750%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.197851, Train Acc:89.250%\n",
      "\t Train Loss:0.108699, Train Acc:92.250%\n",
      "\t Train Loss:0.086561, Train Acc:92.833%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.061262, Train Acc:94.500%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.076554, Train Acc:93.917%\n",
      "\t Train Loss:0.062333, Train Acc:94.083%\n",
      "\t Train Loss:0.061655, Train Acc:93.750%\n",
      "\t Train Loss:0.065922, Train Acc:94.083%\n",
      "\t Train Loss:0.083855, Train Acc:93.250%\n",
      "\t Train Loss:0.042351, Train Acc:95.250%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.032605, Train Acc:95.167%\n",
      "\t Train Loss:0.024702, Train Acc:95.333%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.297324, Train Acc:86.250%\n",
      "\t Train Loss:0.310236, Train Acc:86.333%\n",
      "\t Train Loss:0.083249, Train Acc:93.083%\n",
      "\t Train Loss:0.047396, Train Acc:94.667%\n",
      "\t Train Loss:0.025771, Train Acc:95.583%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.035290, Train Acc:95.250%\n",
      "\t Train Loss:0.016631, Train Acc:95.583%\n",
      "\t Train Loss:0.008979, Train Acc:96.000%\n",
      "\t Weight Saved\n",
      "\t Train Loss:0.014895, Train Acc:95.833%\n",
      "\t Train Loss:0.027319, Train Acc:95.250%\n",
      "\t Train Loss:0.016315, Train Acc:95.583%\n",
      "\t Train Loss:0.015059, Train Acc:95.750%\n",
      "\t Train Loss:0.032940, Train Acc:95.167%\n",
      "\t Train Loss:0.056983, Train Acc:94.333%\n",
      "\t Train Loss:0.176385, Train Acc:89.583%\n",
      "\t Train Loss:0.085542, Train Acc:93.417%\n",
      "\t Train Loss:0.033156, Train Acc:94.833%\n",
      "\t Train Loss:0.049421, Train Acc:94.667%\n",
      "\t Train Loss:0.044871, Train Acc:94.500%\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "modelname = \"MyNet3_0.001_train_2\"\n",
    "torch.manual_seed(420)\n",
    "model3 = MyNet3()\n",
    "PATH  =\"/home/ubuntu/jupyter notebook/\"\n",
    "# model3.load_state_dict(torch.load(\"/home/ubuntu/jupyter notebook/MyNet3_0.001_train.pt\"))\n",
    "print(modelname)\n",
    "\n",
    "net = model3.to(device,non_blocking=True)\n",
    "trainloss3 = full_procedure(model3,100,128,modelname=modelname, PATH=PATH, lr=0.003,alpha=0.99,gamma=0.002,wd=0,tol=10**(-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fca4b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f64b9181",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c6f8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73095e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
